{
  "timestamp": "2026-02-06T15:10:22.787018",
  "description": "exp08: Robustness analysis for exp07 claims",
  "complementarity_sensitivity": [
    {
      "dataset": "TruthfulQA",
      "percentile": 60,
      "n_hallucinations": 164,
      "se_only": 20,
      "se_only_pct": 12.195121951219512,
      "energy_only": 37,
      "energy_only_pct": 22.5609756097561,
      "both_catch": 61,
      "both_catch_pct": 37.19512195121951,
      "neither_catch": 46,
      "neither_catch_pct": 28.04878048780488
    },
    {
      "dataset": "TruthfulQA",
      "percentile": 70,
      "n_hallucinations": 164,
      "se_only": 20,
      "se_only_pct": 12.195121951219512,
      "energy_only": 26,
      "energy_only_pct": 15.853658536585366,
      "both_catch": 89,
      "both_catch_pct": 54.268292682926834,
      "neither_catch": 29,
      "neither_catch_pct": 17.682926829268293
    },
    {
      "dataset": "TruthfulQA",
      "percentile": 80,
      "n_hallucinations": 164,
      "se_only": 16,
      "se_only_pct": 9.75609756097561,
      "energy_only": 29,
      "energy_only_pct": 17.682926829268293,
      "both_catch": 102,
      "both_catch_pct": 62.19512195121951,
      "neither_catch": 17,
      "neither_catch_pct": 10.365853658536585
    },
    {
      "dataset": "TruthfulQA",
      "percentile": 90,
      "n_hallucinations": 164,
      "se_only": 8,
      "se_only_pct": 4.878048780487805,
      "energy_only": 19,
      "energy_only_pct": 11.585365853658537,
      "both_catch": 128,
      "both_catch_pct": 78.04878048780488,
      "neither_catch": 9,
      "neither_catch_pct": 5.487804878048781
    },
    {
      "dataset": "HaluEval",
      "percentile": 60,
      "n_hallucinations": 21,
      "se_only": 3,
      "se_only_pct": 14.285714285714285,
      "energy_only": 4,
      "energy_only_pct": 19.047619047619047,
      "both_catch": 8,
      "both_catch_pct": 38.095238095238095,
      "neither_catch": 6,
      "neither_catch_pct": 28.57142857142857
    },
    {
      "dataset": "HaluEval",
      "percentile": 70,
      "n_hallucinations": 21,
      "se_only": 2,
      "se_only_pct": 9.523809523809524,
      "energy_only": 5,
      "energy_only_pct": 23.809523809523807,
      "both_catch": 9,
      "both_catch_pct": 42.857142857142854,
      "neither_catch": 5,
      "neither_catch_pct": 23.809523809523807
    },
    {
      "dataset": "HaluEval",
      "percentile": 80,
      "n_hallucinations": 21,
      "se_only": 0,
      "se_only_pct": 0.0,
      "energy_only": 5,
      "energy_only_pct": 23.809523809523807,
      "both_catch": 11,
      "both_catch_pct": 52.38095238095239,
      "neither_catch": 5,
      "neither_catch_pct": 23.809523809523807
    },
    {
      "dataset": "HaluEval",
      "percentile": 90,
      "n_hallucinations": 21,
      "se_only": 0,
      "se_only_pct": 0.0,
      "energy_only": 7,
      "energy_only_pct": 33.33333333333333,
      "both_catch": 11,
      "both_catch_pct": 52.38095238095239,
      "neither_catch": 3,
      "neither_catch_pct": 14.285714285714285
    },
    {
      "dataset": "HaluEval-dialogue",
      "percentile": 60,
      "n_hallucinations": 188,
      "se_only": 24,
      "se_only_pct": 12.76595744680851,
      "energy_only": 30,
      "energy_only_pct": 15.957446808510639,
      "both_catch": 83,
      "both_catch_pct": 44.148936170212764,
      "neither_catch": 51,
      "neither_catch_pct": 27.127659574468083
    },
    {
      "dataset": "HaluEval-dialogue",
      "percentile": 70,
      "n_hallucinations": 188,
      "se_only": 25,
      "se_only_pct": 13.297872340425531,
      "energy_only": 37,
      "energy_only_pct": 19.680851063829788,
      "both_catch": 94,
      "both_catch_pct": 50.0,
      "neither_catch": 32,
      "neither_catch_pct": 17.02127659574468
    },
    {
      "dataset": "HaluEval-dialogue",
      "percentile": 80,
      "n_hallucinations": 188,
      "se_only": 17,
      "se_only_pct": 9.042553191489363,
      "energy_only": 23,
      "energy_only_pct": 12.23404255319149,
      "both_catch": 127,
      "both_catch_pct": 67.5531914893617,
      "neither_catch": 21,
      "neither_catch_pct": 11.170212765957446
    },
    {
      "dataset": "HaluEval-dialogue",
      "percentile": 90,
      "n_hallucinations": 188,
      "se_only": 5,
      "se_only_pct": 2.6595744680851063,
      "energy_only": 30,
      "energy_only_pct": 15.957446808510639,
      "both_catch": 139,
      "both_catch_pct": 73.93617021276596,
      "neither_catch": 14,
      "neither_catch_pct": 7.446808510638298
    },
    {
      "dataset": "NaturalQuestions",
      "percentile": 60,
      "n_hallucinations": 114,
      "se_only": 14,
      "se_only_pct": 12.280701754385964,
      "energy_only": 15,
      "energy_only_pct": 13.157894736842104,
      "both_catch": 53,
      "both_catch_pct": 46.49122807017544,
      "neither_catch": 32,
      "neither_catch_pct": 28.07017543859649
    },
    {
      "dataset": "NaturalQuestions",
      "percentile": 70,
      "n_hallucinations": 114,
      "se_only": 9,
      "se_only_pct": 7.894736842105263,
      "energy_only": 22,
      "energy_only_pct": 19.298245614035086,
      "both_catch": 58,
      "both_catch_pct": 50.877192982456144,
      "neither_catch": 25,
      "neither_catch_pct": 21.929824561403507
    },
    {
      "dataset": "NaturalQuestions",
      "percentile": 80,
      "n_hallucinations": 114,
      "se_only": 9,
      "se_only_pct": 7.894736842105263,
      "energy_only": 13,
      "energy_only_pct": 11.403508771929824,
      "both_catch": 78,
      "both_catch_pct": 68.42105263157895,
      "neither_catch": 14,
      "neither_catch_pct": 12.280701754385964
    },
    {
      "dataset": "NaturalQuestions",
      "percentile": 90,
      "n_hallucinations": 114,
      "se_only": 4,
      "se_only_pct": 3.508771929824561,
      "energy_only": 19,
      "energy_only_pct": 16.666666666666664,
      "both_catch": 83,
      "both_catch_pct": 72.80701754385966,
      "neither_catch": 8,
      "neither_catch_pct": 7.017543859649122
    },
    {
      "dataset": "TriviaQA",
      "percentile": 60,
      "n_hallucinations": 110,
      "se_only": 12,
      "se_only_pct": 10.909090909090908,
      "energy_only": 21,
      "energy_only_pct": 19.090909090909093,
      "both_catch": 45,
      "both_catch_pct": 40.909090909090914,
      "neither_catch": 32,
      "neither_catch_pct": 29.09090909090909
    },
    {
      "dataset": "TriviaQA",
      "percentile": 70,
      "n_hallucinations": 110,
      "se_only": 12,
      "se_only_pct": 10.909090909090908,
      "energy_only": 15,
      "energy_only_pct": 13.636363636363635,
      "both_catch": 62,
      "both_catch_pct": 56.36363636363636,
      "neither_catch": 21,
      "neither_catch_pct": 19.090909090909093
    },
    {
      "dataset": "TriviaQA",
      "percentile": 80,
      "n_hallucinations": 110,
      "se_only": 9,
      "se_only_pct": 8.181818181818182,
      "energy_only": 19,
      "energy_only_pct": 17.272727272727273,
      "both_catch": 69,
      "both_catch_pct": 62.727272727272734,
      "neither_catch": 13,
      "neither_catch_pct": 11.818181818181818
    },
    {
      "dataset": "TriviaQA",
      "percentile": 90,
      "n_hallucinations": 110,
      "se_only": 8,
      "se_only_pct": 7.2727272727272725,
      "energy_only": 16,
      "energy_only_pct": 14.545454545454545,
      "both_catch": 83,
      "both_catch_pct": 75.45454545454545,
      "neither_catch": 3,
      "neither_catch_pct": 2.727272727272727
    }
  ],
  "bootstrap_tests": [
    {
      "observed_delta": 0.029471544715447218,
      "bootstrap_mean_delta": 0.029351432844658524,
      "ci_95_lower": -0.015813337053571495,
      "ci_95_upper": 0.07412970847327567,
      "p_value": 0.0946,
      "significant_at_005": false,
      "significant_at_010": true,
      "n_bootstrap_valid": 5000,
      "dataset": "TruthfulQA",
      "tau": 0.5255307469172573,
      "se_auroc": 0.6132283197831978,
      "cascade_auroc": 0.642699864498645,
      "tau_source": "best_per_dataset"
    },
    {
      "observed_delta": 0.029471544715447218,
      "bootstrap_mean_delta": 0.029351432844658524,
      "ci_95_lower": -0.015813337053571495,
      "ci_95_upper": 0.07412970847327567,
      "p_value": 0.0946,
      "significant_at_005": false,
      "significant_at_010": true,
      "n_bootstrap_valid": 5000,
      "dataset": "TruthfulQA",
      "tau": 0.526,
      "se_auroc": 0.6132283197831978,
      "cascade_auroc": 0.642699864498645,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "observed_delta": 0.07382282521947325,
      "bootstrap_mean_delta": 0.07301155025385075,
      "ci_95_lower": -0.06222569444444451,
      "ci_95_upper": 0.22042799654180043,
      "p_value": 0.1546,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "HaluEval",
      "tau": 1.3321790402101223,
      "se_auroc": 0.5400372439478585,
      "cascade_auroc": 0.6138600691673317,
      "tau_source": "best_per_dataset"
    },
    {
      "observed_delta": 0.0544027666932696,
      "bootstrap_mean_delta": 0.05351769177650182,
      "ci_95_lower": -0.08965884000805971,
      "ci_95_upper": 0.20429684091679925,
      "p_value": 0.2344,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "HaluEval",
      "tau": 0.526,
      "se_auroc": 0.5400372439478585,
      "cascade_auroc": 0.5944400106411281,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "observed_delta": -0.002437943262411313,
      "bootstrap_mean_delta": -0.003203329769632754,
      "ci_95_lower": -0.13035232741115083,
      "ci_95_upper": 0.0995311114804962,
      "p_value": 0.5056,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "HaluEval-dialogue",
      "tau": 0.5255307469172573,
      "se_auroc": 0.5986258865248227,
      "cascade_auroc": 0.5961879432624114,
      "tau_source": "best_per_dataset"
    },
    {
      "observed_delta": -0.002437943262411313,
      "bootstrap_mean_delta": -0.003203329769632754,
      "ci_95_lower": -0.13035232741115083,
      "ci_95_upper": 0.0995311114804962,
      "p_value": 0.5056,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "HaluEval-dialogue",
      "tau": 0.526,
      "se_auroc": 0.5986258865248227,
      "cascade_auroc": 0.5961879432624114,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "observed_delta": 0.02611179110567119,
      "bootstrap_mean_delta": 0.02524465667801616,
      "ci_95_lower": -0.04949333333333328,
      "ci_95_upper": 0.09958411024912323,
      "p_value": 0.2518,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "NaturalQuestions",
      "tau": 1.6094379124341005,
      "se_auroc": 0.6355569155446756,
      "cascade_auroc": 0.6616687066503468,
      "tau_source": "best_per_dataset"
    },
    {
      "observed_delta": -0.012902896776825767,
      "bootstrap_mean_delta": -0.013348046436020633,
      "ci_95_lower": -0.045507265123697176,
      "ci_95_upper": 0.01698341335006778,
      "p_value": 0.1952,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "NaturalQuestions",
      "tau": 0.526,
      "se_auroc": 0.6355569155446756,
      "cascade_auroc": 0.6226540187678499,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "observed_delta": -0.00035353535353532806,
      "bootstrap_mean_delta": -0.0005188139665441746,
      "ci_95_lower": -0.04937914624338824,
      "ci_95_upper": 0.04820146345913245,
      "p_value": 0.503,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "TriviaQA",
      "tau": 0.0,
      "se_auroc": 0.6686868686868687,
      "cascade_auroc": 0.6683333333333333,
      "tau_source": "best_per_dataset"
    },
    {
      "observed_delta": -0.00121212121212122,
      "bootstrap_mean_delta": -0.0014588283666853359,
      "ci_95_lower": -0.05022339314331504,
      "ci_95_upper": 0.04720251325167752,
      "p_value": 0.4756,
      "significant_at_005": false,
      "significant_at_010": false,
      "n_bootstrap_valid": 5000,
      "dataset": "TriviaQA",
      "tau": 0.526,
      "se_auroc": 0.6686868686868687,
      "cascade_auroc": 0.6674747474747474,
      "tau_source": "cross_dataset_0.526"
    }
  ],
  "zero_se_single_cluster_verification": [
    {
      "dataset": "TruthfulQA",
      "n_total": 200,
      "n_se_zero_eps001": 38,
      "n_single_cluster": 38,
      "n_overlap": 38,
      "exact_match": true,
      "se_zero_but_multi_cluster": 0,
      "single_cluster_but_se_nonzero": 0,
      "cluster_distribution": {
        "1": {
          "total": 38,
          "hall": 28,
          "normal": 10
        },
        "2": {
          "total": 36,
          "hall": 27,
          "normal": 9
        },
        "3": {
          "total": 42,
          "hall": 36,
          "normal": 6
        },
        "4": {
          "total": 48,
          "hall": 39,
          "normal": 9
        },
        "5": {
          "total": 36,
          "hall": 34,
          "normal": 2
        }
      }
    },
    {
      "dataset": "HaluEval",
      "n_total": 200,
      "n_se_zero_eps001": 107,
      "n_single_cluster": 107,
      "n_overlap": 107,
      "exact_match": true,
      "se_zero_but_multi_cluster": 0,
      "single_cluster_but_se_nonzero": 0,
      "cluster_distribution": {
        "1": {
          "total": 107,
          "hall": 10,
          "normal": 97
        },
        "2": {
          "total": 54,
          "hall": 7,
          "normal": 47
        },
        "3": {
          "total": 20,
          "hall": 0,
          "normal": 20
        },
        "4": {
          "total": 17,
          "hall": 3,
          "normal": 14
        },
        "5": {
          "total": 2,
          "hall": 1,
          "normal": 1
        }
      }
    },
    {
      "dataset": "HaluEval-dialogue",
      "n_total": 200,
      "n_se_zero_eps001": 49,
      "n_single_cluster": 49,
      "n_overlap": 49,
      "exact_match": true,
      "se_zero_but_multi_cluster": 0,
      "single_cluster_but_se_nonzero": 0,
      "cluster_distribution": {
        "1": {
          "total": 49,
          "hall": 44,
          "normal": 5
        },
        "2": {
          "total": 39,
          "hall": 37,
          "normal": 2
        },
        "3": {
          "total": 43,
          "hall": 42,
          "normal": 1
        },
        "4": {
          "total": 44,
          "hall": 40,
          "normal": 4
        },
        "5": {
          "total": 25,
          "hall": 25,
          "normal": 0
        }
      }
    },
    {
      "dataset": "NaturalQuestions",
      "n_total": 200,
      "n_se_zero_eps001": 34,
      "n_single_cluster": 34,
      "n_overlap": 34,
      "exact_match": true,
      "se_zero_but_multi_cluster": 0,
      "single_cluster_but_se_nonzero": 0,
      "cluster_distribution": {
        "1": {
          "total": 34,
          "hall": 11,
          "normal": 23
        },
        "2": {
          "total": 42,
          "hall": 21,
          "normal": 21
        },
        "3": {
          "total": 33,
          "hall": 22,
          "normal": 11
        },
        "4": {
          "total": 37,
          "hall": 22,
          "normal": 15
        },
        "5": {
          "total": 54,
          "hall": 38,
          "normal": 16
        }
      }
    },
    {
      "dataset": "TriviaQA",
      "n_total": 200,
      "n_se_zero_eps001": 58,
      "n_single_cluster": 58,
      "n_overlap": 58,
      "exact_match": true,
      "se_zero_but_multi_cluster": 0,
      "single_cluster_but_se_nonzero": 0,
      "cluster_distribution": {
        "1": {
          "total": 58,
          "hall": 19,
          "normal": 39
        },
        "2": {
          "total": 36,
          "hall": 17,
          "normal": 19
        },
        "3": {
          "total": 42,
          "hall": 28,
          "normal": 14
        },
        "4": {
          "total": 31,
          "hall": 25,
          "normal": 6
        },
        "5": {
          "total": 33,
          "hall": 21,
          "normal": 12
        }
      }
    }
  ],
  "rank_normalization_comparison": [
    {
      "dataset": "TruthfulQA",
      "tau": 0.5255307469172573,
      "se_auroc_minmax": 0.6132283197831978,
      "se_auroc_rank": 0.6512533875338753,
      "cascade_auroc_minmax": 0.642699864498645,
      "cascade_auroc_rank": 0.6508299457994579,
      "delta_minmax": 0.029471544715447218,
      "delta_rank": -0.00042344173441744015,
      "rank_vs_minmax_consistent": false
    },
    {
      "dataset": "TruthfulQA",
      "tau": 0.526,
      "se_auroc_minmax": 0.6132283197831978,
      "se_auroc_rank": 0.6512533875338753,
      "cascade_auroc_minmax": 0.642699864498645,
      "cascade_auroc_rank": 0.6508299457994579,
      "delta_minmax": 0.029471544715447218,
      "delta_rank": -0.00042344173441744015,
      "rank_vs_minmax_consistent": false,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "dataset": "HaluEval",
      "tau": 1.3321790402101223,
      "se_auroc_minmax": 0.5400372439478585,
      "se_auroc_rank": 0.518488959829742,
      "cascade_auroc_minmax": 0.6138600691673317,
      "cascade_auroc_rank": 0.6142591114658154,
      "delta_minmax": 0.07382282521947325,
      "delta_rank": 0.09577015163607339,
      "rank_vs_minmax_consistent": true
    },
    {
      "dataset": "HaluEval",
      "tau": 0.526,
      "se_auroc_minmax": 0.5400372439478585,
      "se_auroc_rank": 0.518488959829742,
      "cascade_auroc_minmax": 0.5944400106411281,
      "cascade_auroc_rank": 0.6169193934557062,
      "delta_minmax": 0.0544027666932696,
      "delta_rank": 0.09843043362596426,
      "rank_vs_minmax_consistent": true,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "dataset": "HaluEval-dialogue",
      "tau": 0.5255307469172573,
      "se_auroc_minmax": 0.5986258865248227,
      "se_auroc_rank": 0.5722517730496454,
      "cascade_auroc_minmax": 0.5961879432624114,
      "cascade_auroc_rank": 0.5718085106382979,
      "delta_minmax": -0.002437943262411313,
      "delta_rank": -0.00044326241134751143,
      "rank_vs_minmax_consistent": true
    },
    {
      "dataset": "HaluEval-dialogue",
      "tau": 0.526,
      "se_auroc_minmax": 0.5986258865248227,
      "se_auroc_rank": 0.5722517730496454,
      "cascade_auroc_minmax": 0.5961879432624114,
      "cascade_auroc_rank": 0.5718085106382979,
      "delta_minmax": -0.002437943262411313,
      "delta_rank": -0.00044326241134751143,
      "rank_vs_minmax_consistent": true,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "dataset": "NaturalQuestions",
      "tau": 1.6094379124341005,
      "se_auroc_minmax": 0.6355569155446756,
      "se_auroc_rank": 0.6286209710322317,
      "cascade_auroc_minmax": 0.6616687066503468,
      "cascade_auroc_rank": 0.6616687066503468,
      "delta_minmax": 0.02611179110567119,
      "delta_rank": 0.03304773561811514,
      "rank_vs_minmax_consistent": true
    },
    {
      "dataset": "NaturalQuestions",
      "tau": 0.526,
      "se_auroc_minmax": 0.6355569155446756,
      "se_auroc_rank": 0.6286209710322317,
      "cascade_auroc_minmax": 0.6226540187678499,
      "cascade_auroc_rank": 0.6159220726234191,
      "delta_minmax": -0.012902896776825767,
      "delta_rank": -0.012698898408812553,
      "rank_vs_minmax_consistent": true,
      "tau_source": "cross_dataset_0.526"
    },
    {
      "dataset": "TriviaQA",
      "tau": 0.0,
      "se_auroc_minmax": 0.6686868686868687,
      "se_auroc_rank": 0.6606060606060606,
      "cascade_auroc_minmax": 0.6683333333333333,
      "cascade_auroc_rank": 0.671969696969697,
      "delta_minmax": -0.00035353535353532806,
      "delta_rank": 0.011363636363636354,
      "rank_vs_minmax_consistent": false
    },
    {
      "dataset": "TriviaQA",
      "tau": 0.526,
      "se_auroc_minmax": 0.6686868686868687,
      "se_auroc_rank": 0.6606060606060606,
      "cascade_auroc_minmax": 0.6674747474747474,
      "cascade_auroc_rank": 0.6624747474747474,
      "delta_minmax": -0.00121212121212122,
      "delta_rank": 0.0018686868686867975,
      "rank_vs_minmax_consistent": false,
      "tau_source": "cross_dataset_0.526"
    }
  ]
}